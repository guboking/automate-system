#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦åˆ†æç§‘æŠ€æ¿å—çš„ç»†åˆ†é¢†åŸŸ
"""

import json
import re
from collections import defaultdict

class TechSubsectorAnalyzer:
    def __init__(self, reports_file):
        with open(reports_file, 'r', encoding='utf-8') as f:
            self.reports = json.load(f)

        # ç§‘æŠ€ç»†åˆ†æ¿å—å…³é”®è¯ï¼ˆæ›´è¯¦ç»†ï¼‰
        self.tech_subsectors = {
            "äººå·¥æ™ºèƒ½/AI": [
                "äººå·¥æ™ºèƒ½", "AI", "å¤§æ¨¡å‹", "ChatGPT", "GPT", "æ·±åº¦å­¦ä¹ ",
                "æœºå™¨å­¦ä¹ ", "ç®—æ³•", "æ™ºèƒ½ç®—åŠ›", "ç®—åŠ›", "AIåº”ç”¨", "AIGC",
                "è‡ªç„¶è¯­è¨€", "è®¡ç®—æœºè§†è§‰", "è¯­éŸ³è¯†åˆ«", "æ™ºèƒ½åŒ–"
            ],
            "åŠå¯¼ä½“/èŠ¯ç‰‡": [
                "åŠå¯¼ä½“", "èŠ¯ç‰‡", "é›†æˆç”µè·¯", "æ™¶åœ†", "å…‰åˆ»æœº", "EDA",
                "å­˜å‚¨èŠ¯ç‰‡", "åŠŸç‡åŠå¯¼ä½“", "MCU", "GPU", "CPU", "å°è£…æµ‹è¯•",
                "å›½äº§æ›¿ä»£", "èŠ¯ç‰‡è®¾è®¡", "æ™¶åœ†åˆ¶é€ ", "ASML", "å°ç§¯ç”µ"
            ],
            "äº‘è®¡ç®—": [
                "äº‘è®¡ç®—", "äº‘æœåŠ¡", "äº‘åŸºç¡€è®¾æ–½", "æ•°æ®ä¸­å¿ƒ", "æœåŠ¡å™¨",
                "IDC", "ç®—åŠ›ä¸­å¿ƒ", "è¾¹ç¼˜è®¡ç®—", "æ··åˆäº‘", "ç§æœ‰äº‘", "å…¬æœ‰äº‘",
                "äº‘åŸç”Ÿ", "SaaS", "PaaS", "IaaS"
            ],
            "è½¯ä»¶": [
                "è½¯ä»¶", "æ“ä½œç³»ç»Ÿ", "æ•°æ®åº“", "ä¸­é—´ä»¶", "å·¥ä¸šè½¯ä»¶",
                "åŠå…¬è½¯ä»¶", "ERP", "CRM", "CAD", "CAE", "PLM",
                "ä¿¡åˆ›", "å›½äº§è½¯ä»¶", "é‡‘è¶", "ç”¨å‹", "é¸¿è’™"
            ],
            "ç½‘ç»œå®‰å…¨": [
                "ç½‘ç»œå®‰å…¨", "ä¿¡æ¯å®‰å…¨", "æ•°æ®å®‰å…¨", "ç½‘ç»œå®‰å…¨", "é˜²ç«å¢™",
                "æ€åŠ¿æ„ŸçŸ¥", "å®‰å…¨é˜²æŠ¤", "ç½‘å®‰", "ç­‰ä¿", "å¯†ç "
            ],
            "5G/é€šä¿¡": [
                "5G", "6G", "é€šä¿¡", "åŸºç«™", "å…‰é€šä¿¡", "å…‰æ¨¡å—", "å…‰çº¤",
                "é€šä¿¡è®¾å¤‡", "å¤©çº¿", "å°„é¢‘", "ç‰©è”ç½‘", "IoT", "èœ‚çª"
            ],
            "å¤§æ•°æ®": [
                "å¤§æ•°æ®", "æ•°æ®åˆ†æ", "æ•°æ®æ²»ç†", "æ•°æ®ä¸­å°",
                "å•†ä¸šæ™ºèƒ½", "BI", "æ•°æ®æŒ–æ˜", "æ•°æ®å¯è§†åŒ–"
            ],
            "å·¥ä¸šäº’è”ç½‘": [
                "å·¥ä¸šäº’è”ç½‘", "å·¥ä¸š4.0", "æ™ºèƒ½åˆ¶é€ ", "æ•°å­—å­ªç”Ÿ",
                "MES", "æ™ºèƒ½å·¥å‚", "æŸ”æ€§åˆ¶é€ ", "å·¥ä¸šç‰©è”ç½‘"
            ],
            "ä¿¡åˆ›/å›½äº§åŒ–": [
                "ä¿¡åˆ›", "å›½äº§åŒ–", "è‡ªä¸»å¯æ§", "å»IOE", "å›½äº§æ›¿ä»£",
                "æ“ä½œç³»ç»Ÿå›½äº§åŒ–", "èŠ¯ç‰‡å›½äº§åŒ–", "è½¯ä»¶å›½äº§åŒ–"
            ],
            "å«æ˜Ÿäº’è”ç½‘": [
                "å«æ˜Ÿäº’è”ç½‘", "ä½è½¨å«æ˜Ÿ", "æ˜Ÿé“¾", "å«æ˜Ÿé€šä¿¡",
                "åŒ—æ–—", "å¯¼èˆª", "é¥æ„Ÿå«æ˜Ÿ"
            ],
            "æ¶ˆè´¹ç”µå­": [
                "æ¶ˆè´¹ç”µå­", "æ™ºèƒ½æ‰‹æœº", "å¯ç©¿æˆ´", "TWS", "VR", "AR",
                "å…ƒå®‡å®™", "æ™ºèƒ½éŸ³ç®±", "å¹³æ¿ç”µè„‘", "æ™ºèƒ½å®¶å±…"
            ],
            "é‡å­è®¡ç®—": [
                "é‡å­è®¡ç®—", "é‡å­é€šä¿¡", "é‡å­ç§‘æŠ€", "é‡å­èŠ¯ç‰‡"
            ]
        }

        # çŸ¥åç§‘æŠ€å…¬å¸/è‚¡ç¥¨
        self.tech_companies = {
            # AI
            "ç§‘å¤§è®¯é£": "002230",
            "å¯’æ­¦çºª": "688256",
            "æµ·å…‰ä¿¡æ¯": "688041",
            "æ‹“å°”æ€": "300229",

            # åŠå¯¼ä½“
            "ä¸­èŠ¯å›½é™…": "688981",
            "åŒ—æ–¹ååˆ›": "002371",
            "åè™¹å…¬å¸": "688347",
            "éŸ¦å°”è‚¡ä»½": "603501",
            "å…†æ˜“åˆ›æ–°": "603986",
            "å“èƒœå¾®": "300782",
            "ä¸‰å®‰å…‰ç”µ": "600703",
            "é•¿ç”µç§‘æŠ€": "600584",
            "ç´«å…‰å›½å¾®": "002049",

            # äº‘è®¡ç®—/æœåŠ¡å™¨
            "æµªæ½®ä¿¡æ¯": "000977",
            "ç´«å…‰è‚¡ä»½": "000938",
            "ä¸­ç§‘æ›™å…‰": "603019",
            "å®ä¿¡è½¯ä»¶": "600845",

            # è½¯ä»¶
            "ç”¨å‹ç½‘ç»œ": "600588",
            "é‡‘è¶å›½é™…": "HK00268",
            "å¹¿è”è¾¾": "002410",
            "æ’ç”Ÿç”µå­": "600570",
            "ä¸­æœ›è½¯ä»¶": "688083",

            # 5G/é€šä¿¡
            "ä¸­å…´é€šè®¯": "000063",
            "çƒ½ç«é€šä¿¡": "600498",
            "ä¸­é™…æ—­åˆ›": "300308",
            "æ–°æ˜“ç››": "300502",
            "å¤©å­šé€šä¿¡": "300394",

            # ç½‘ç»œå®‰å…¨
            "æ·±ä¿¡æœ": "300454",
            "å¯æ˜æ˜Ÿè¾°": "002439",
            "å¥‡å®‰ä¿¡": "688561",
            "å®‰æ’ä¿¡æ¯": "688023",

            # æ¶ˆè´¹ç”µå­
            "ç«‹è®¯ç²¾å¯†": "002475",
            "æ­Œå°”è‚¡ä»½": "002241",
            "äº¬ä¸œæ–¹A": "000725",
            "TCLç§‘æŠ€": "000100",
        }

        self.subsector_data = defaultdict(lambda: {
            "count": 0,
            "reports": [],
            "context": [],
            "related_stocks": set(),
            "keywords_matched": set()
        })

    def analyze_sentiment(self, text):
        """åˆ†ææƒ…æ„Ÿå€¾å‘"""
        positive_words = [
            "çœ‹å¥½", "æ¨è", "ä¹°å…¥", "å¢æŒ", "è¶…é…", "é…ç½®", "æœºä¼š", "ä¸Šæ¶¨", "å¼ºåŠ¿",
            "çªç ´", "åå¼¹", "åº•éƒ¨", "ä½ä¼°", "ä¼˜è´¨", "é¾™å¤´", "æ ¸å¿ƒ", "é‡ç‚¹", "æŒç»­",
            "å—ç›Š", "æ™¯æ°”", "é«˜å¢é•¿", "ç¡®å®šæ€§", "å¯æœŸ", "ç§¯æ", "è¶…é¢„æœŸ", "åŠ é€Ÿ",
            "é¢†å…ˆ", "åˆ›æ–°", "çªç ´"
        ]

        negative_words = [
            "å›è°ƒ", "ä¸‹è·Œ", "é£é™©", "è°¨æ…", "å‡æŒ", "å–å‡º", "å¼±åŠ¿", "å‹åŠ›",
            "é«˜ä¼°", "æ³¡æ²«", "ææ…Œ", "è­¦æƒ•", "é¿å…", "ä¸‹è¡Œ", "ç–²è½¯", "æ”¾ç¼“"
        ]

        pos_count = sum(1 for word in positive_words if word in text)
        neg_count = sum(1 for word in negative_words if word in text)

        total = pos_count + neg_count
        if total == 0:
            return 0
        return (pos_count - neg_count) / total

    def extract_context(self, content, keyword, window=100):
        """æå–å…³é”®è¯å‘¨å›´çš„ä¸Šä¸‹æ–‡"""
        pattern = f'.{{0,{window}}}{re.escape(keyword)}.{{0,{window}}}'
        matches = re.findall(pattern, content)
        return matches[:3]  # æœ€å¤šè¿”å›3ä¸ªä¸Šä¸‹æ–‡

    def analyze_all_reports(self):
        """åˆ†ææ‰€æœ‰æŠ¥å‘Šä¸­çš„ç§‘æŠ€ç»†åˆ†é¢†åŸŸ"""
        for filename, data in self.reports.items():
            content = data['content']

            # éå†æ¯ä¸ªç»†åˆ†é¢†åŸŸ
            for subsector, keywords in self.tech_subsectors.items():
                for keyword in keywords:
                    if keyword in content:
                        count = content.count(keyword)
                        self.subsector_data[subsector]["count"] += count
                        self.subsector_data[subsector]["reports"].append(filename)
                        self.subsector_data[subsector]["keywords_matched"].add(keyword)

                        # æå–ä¸Šä¸‹æ–‡
                        contexts = self.extract_context(content, keyword)
                        self.subsector_data[subsector]["context"].extend(contexts)

            # è¯†åˆ«æåŠçš„ç§‘æŠ€å…¬å¸
            for company, code in self.tech_companies.items():
                if company in content or code in content:
                    # åˆ¤æ–­è¿™ä¸ªå…¬å¸å±äºå“ªä¸ªç»†åˆ†é¢†åŸŸ
                    self._assign_company_to_subsector(company, code, content)

    def _assign_company_to_subsector(self, company, code, content):
        """å°†å…¬å¸åˆ†é…åˆ°å¯¹åº”çš„ç»†åˆ†é¢†åŸŸ"""
        # ç®€å•çš„æ˜ å°„è§„åˆ™
        company_sector_map = {
            # AI
            "ç§‘å¤§è®¯é£": "äººå·¥æ™ºèƒ½/AI", "å¯’æ­¦çºª": "äººå·¥æ™ºèƒ½/AI",
            "æµ·å…‰ä¿¡æ¯": "äººå·¥æ™ºèƒ½/AI", "æ‹“å°”æ€": "äººå·¥æ™ºèƒ½/AI",

            # åŠå¯¼ä½“
            "ä¸­èŠ¯å›½é™…": "åŠå¯¼ä½“/èŠ¯ç‰‡", "åŒ—æ–¹ååˆ›": "åŠå¯¼ä½“/èŠ¯ç‰‡",
            "åè™¹å…¬å¸": "åŠå¯¼ä½“/èŠ¯ç‰‡", "éŸ¦å°”è‚¡ä»½": "åŠå¯¼ä½“/èŠ¯ç‰‡",
            "å…†æ˜“åˆ›æ–°": "åŠå¯¼ä½“/èŠ¯ç‰‡", "å“èƒœå¾®": "åŠå¯¼ä½“/èŠ¯ç‰‡",
            "ä¸‰å®‰å…‰ç”µ": "åŠå¯¼ä½“/èŠ¯ç‰‡", "é•¿ç”µç§‘æŠ€": "åŠå¯¼ä½“/èŠ¯ç‰‡",
            "ç´«å…‰å›½å¾®": "åŠå¯¼ä½“/èŠ¯ç‰‡",

            # äº‘è®¡ç®—
            "æµªæ½®ä¿¡æ¯": "äº‘è®¡ç®—", "ç´«å…‰è‚¡ä»½": "äº‘è®¡ç®—",
            "ä¸­ç§‘æ›™å…‰": "äº‘è®¡ç®—", "å®ä¿¡è½¯ä»¶": "äº‘è®¡ç®—",

            # è½¯ä»¶
            "ç”¨å‹ç½‘ç»œ": "è½¯ä»¶", "é‡‘è¶å›½é™…": "è½¯ä»¶",
            "å¹¿è”è¾¾": "è½¯ä»¶", "æ’ç”Ÿç”µå­": "è½¯ä»¶",
            "ä¸­æœ›è½¯ä»¶": "è½¯ä»¶",

            # 5G/é€šä¿¡
            "ä¸­å…´é€šè®¯": "5G/é€šä¿¡", "çƒ½ç«é€šä¿¡": "5G/é€šä¿¡",
            "ä¸­é™…æ—­åˆ›": "5G/é€šä¿¡", "æ–°æ˜“ç››": "5G/é€šä¿¡",
            "å¤©å­šé€šä¿¡": "5G/é€šä¿¡",

            # ç½‘ç»œå®‰å…¨
            "æ·±ä¿¡æœ": "ç½‘ç»œå®‰å…¨", "å¯æ˜æ˜Ÿè¾°": "ç½‘ç»œå®‰å…¨",
            "å¥‡å®‰ä¿¡": "ç½‘ç»œå®‰å…¨", "å®‰æ’ä¿¡æ¯": "ç½‘ç»œå®‰å…¨",

            # æ¶ˆè´¹ç”µå­
            "ç«‹è®¯ç²¾å¯†": "æ¶ˆè´¹ç”µå­", "æ­Œå°”è‚¡ä»½": "æ¶ˆè´¹ç”µå­",
            "äº¬ä¸œæ–¹A": "æ¶ˆè´¹ç”µå­", "TCLç§‘æŠ€": "æ¶ˆè´¹ç”µå­",
        }

        if company in company_sector_map:
            subsector = company_sector_map[company]
            self.subsector_data[subsector]["related_stocks"].add(f"{company}({code})")

    def calculate_scores(self):
        """è®¡ç®—å„ç»†åˆ†é¢†åŸŸçš„è¯„åˆ†"""
        scores = {}

        max_count = max([v["count"] for v in self.subsector_data.values()]) if self.subsector_data else 1
        total_reports = len(self.reports)

        for subsector, data in self.subsector_data.items():
            if data["count"] == 0:
                continue

            # æåŠé¢‘ç‡å¾—åˆ† (40åˆ†)
            frequency_score = (data["count"] / max_count) * 40

            # æŠ¥å‘Šè¦†ç›–åº¦ (30åˆ†)
            unique_reports = len(set(data["reports"]))
            coverage_score = (unique_reports / total_reports) * 30

            # æƒ…æ„Ÿåˆ†æ•° (30åˆ†)
            total_sentiment = sum([self.analyze_sentiment(ctx) for ctx in data["context"]])
            avg_sentiment = total_sentiment / len(data["context"]) if data["context"] else 0
            sentiment_score = ((avg_sentiment + 1) / 2) * 30

            total_score = frequency_score + coverage_score + sentiment_score

            scores[subsector] = {
                "score": round(total_score, 2),
                "frequency": data["count"],
                "coverage": unique_reports,
                "sentiment": round(avg_sentiment, 2),
                "keywords": list(data["keywords_matched"]),
                "stocks": list(data["related_stocks"]),
                "key_contexts": data["context"][:3]
            }

        return scores

    def generate_report(self):
        """ç”ŸæˆæŠ¥å‘Š"""
        self.analyze_all_reports()
        scores = self.calculate_scores()

        # æ’åº
        sorted_subsectors = sorted(scores.items(), key=lambda x: x[1]["score"], reverse=True)

        return {
            "total_subsectors": len(sorted_subsectors),
            "subsector_rankings": sorted_subsectors,
            "summary": self._generate_summary(sorted_subsectors)
        }

    def _generate_summary(self, sorted_subsectors):
        """ç”Ÿæˆæ‘˜è¦"""
        if not sorted_subsectors:
            return "æœªè¯†åˆ«åˆ°ç§‘æŠ€ç»†åˆ†é¢†åŸŸ"

        top3 = [s[0] for s in sorted_subsectors[:3]]
        return f"ç§‘æŠ€æ¿å—æœ€çƒ­é—¨çš„ä¸‰å¤§ç»†åˆ†é¢†åŸŸï¼š{', '.join(top3)}"


def generate_markdown_report(result):
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    md = []

    md.append("# ğŸ”¬ ç§‘æŠ€æ¿å—ç»†åˆ†é¢†åŸŸæ·±åº¦åˆ†æ\n")
    md.append(f"**åˆ†ææ—¶é—´**: {__import__('datetime').datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n")
    md.append("---\n")

    md.append("## ä¸€ã€æ¦‚è§ˆ\n")
    md.append(f"- **è¯†åˆ«ç»†åˆ†é¢†åŸŸæ•°é‡**: {result['total_subsectors']} ä¸ª")
    md.append(f"- **æ ¸å¿ƒå‘ç°**: {result['summary']}\n")

    md.append("\n## äºŒã€ç»†åˆ†é¢†åŸŸè¯„åˆ†æ’è¡Œï¼ˆ100åˆ†åˆ¶ï¼‰\n")
    md.append("| æ’å | ç»†åˆ†é¢†åŸŸ | ç»¼åˆè¯„åˆ† | æåŠæ¬¡æ•° | æŠ¥å‘Šè¦†ç›– | æƒ…æ„Ÿåˆ†æ•° | è¯„çº§ |")
    md.append("|------|---------|---------|---------|---------|---------|------|")

    for i, (subsector, data) in enumerate(result['subsector_rankings'], 1):
        rating = get_rating(data['score'])
        md.append(f"| {i} | **{subsector}** | {data['score']} | {data['frequency']} | "
                  f"{data['coverage']}/13 | {data['sentiment']:.2f} | {rating} |")

    md.append("\n### è¯„çº§è¯´æ˜")
    md.append("- **A+** (90-100åˆ†): æåŠ›æ¨èï¼Œè¡Œä¸šçƒ­ç‚¹")
    md.append("- **A** (80-89åˆ†): å¼ºçƒˆæ¨èï¼Œé«˜æ™¯æ°”åº¦")
    md.append("- **B+** (70-79åˆ†): æ¨èé…ç½®")
    md.append("- **B** (60-69åˆ†): å€¼å¾—å…³æ³¨")
    md.append("- **C** (50-59åˆ†): è§‚æœ›")
    md.append("- **D** (50åˆ†ä»¥ä¸‹): è°¨æ…\n")

    md.append("\n## ä¸‰ã€å„ç»†åˆ†é¢†åŸŸè¯¦ç»†åˆ†æ\n")

    for i, (subsector, data) in enumerate(result['subsector_rankings'][:10], 1):  # Top 10
        md.append(f"\n### {i}. {subsector}")
        md.append(f"**ç»¼åˆè¯„åˆ†**: {data['score']} | **è¯„çº§**: {get_rating(data['score'])}\n")

        # åŸºæœ¬æŒ‡æ ‡
        md.append("#### ğŸ“Š å…³é”®æŒ‡æ ‡")
        md.append(f"- **æåŠæ¬¡æ•°**: {data['frequency']} æ¬¡")
        md.append(f"- **æŠ¥å‘Šè¦†ç›–**: {data['coverage']}/13 ä»½")
        md.append(f"- **å¸‚åœºæƒ…ç»ª**: {'ç§¯æ ğŸ“ˆ' if data['sentiment'] > 0.2 else 'ä¸­æ€§ â¡ï¸' if data['sentiment'] > -0.1 else 'è°¨æ… ğŸ“‰'}")
        md.append(f"- **æƒ…æ„Ÿåˆ†æ•°**: {data['sentiment']:.2f}\n")

        # ç›¸å…³è‚¡ç¥¨
        if data['stocks']:
            md.append("#### ğŸ¯ ç›¸å…³æ ‡çš„")
            for stock in data['stocks']:
                md.append(f"- {stock}")
            md.append("")

        # åŒ¹é…å…³é”®è¯
        if data['keywords']:
            md.append("#### ğŸ”‘ å…³é”®è¯")
            keywords_str = "ã€".join(list(data['keywords'])[:10])
            md.append(f"{keywords_str}\n")

        # æ ¸å¿ƒè§‚ç‚¹
        if data['key_contexts']:
            md.append("#### ğŸ’¡ æ ¸å¿ƒè§‚ç‚¹æ‘˜å½•")
            for j, ctx in enumerate(data['key_contexts'][:2], 1):
                cleaned_ctx = ctx.strip()[:150]  # é™åˆ¶é•¿åº¦
                if cleaned_ctx:
                    md.append(f"{j}. {cleaned_ctx}...")
            md.append("")

        md.append("---\n")

    md.append("\n## å››ã€æŠ•èµ„å»ºè®®\n")

    top_tier = [s[0] for s in result['subsector_rankings'] if s[1]['score'] >= 70]
    mid_tier = [s[0] for s in result['subsector_rankings'] if 60 <= s[1]['score'] < 70]

    if top_tier:
        md.append(f"### âœ… é‡ç‚¹é…ç½®é¢†åŸŸ (è¯„åˆ†â‰¥70)")
        for subsector in top_tier:
            md.append(f"- **{subsector}**")
        md.append("")

    if mid_tier:
        md.append(f"### ğŸ‘€ å…³æ³¨é¢†åŸŸ (è¯„åˆ†60-70)")
        for subsector in mid_tier:
            md.append(f"- {subsector}")
        md.append("")

    md.append("\n### ğŸ“ˆ é…ç½®å»ºè®®")
    md.append("- **æ ¸å¿ƒæŒä»“**: é€‰æ‹©è¯„åˆ†æœ€é«˜çš„2-3ä¸ªç»†åˆ†é¢†åŸŸ")
    md.append("- **å«æ˜Ÿé…ç½®**: é€‚å½“å¸ƒå±€1-2ä¸ªä¸­ç­‰è¯„åˆ†é¢†åŸŸ")
    md.append("- **åˆ†æ•£é£é™©**: é¿å…è¿‡åº¦é›†ä¸­å•ä¸€ç»†åˆ†é¢†åŸŸ")
    md.append("- **åŠ¨æ€è°ƒæ•´**: å…³æ³¨æ”¿ç­–å˜åŒ–å’ŒæŠ€æœ¯çªç ´\n")

    md.append("\n---\n")
    md.append("*æœ¬æŠ¥å‘ŠåŸºäºAIåˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒ*")

    return '\n'.join(md)


def get_rating(score):
    """è¯„çº§"""
    if score >= 90:
        return "A+"
    elif score >= 80:
        return "A"
    elif score >= 70:
        return "B+"
    elif score >= 60:
        return "B"
    elif score >= 50:
        return "C"
    else:
        return "D"


def main():
    print("å¼€å§‹åˆ†æç§‘æŠ€æ¿å—ç»†åˆ†é¢†åŸŸ...\n")

    analyzer = TechSubsectorAnalyzer("/home/user/automate-system/extracted_reports.json")
    result = analyzer.generate_report()

    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    markdown = generate_markdown_report(result)

    # ä¿å­˜æŠ¥å‘Š
    output_file = "/home/user/automate-system/ç§‘æŠ€æ¿å—ç»†åˆ†åˆ†æ.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(markdown)

    # ä¿å­˜ JSON
    json_file = "/home/user/automate-system/tech_subsector_analysis.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_result = {
            "total_subsectors": result['total_subsectors'],
            "summary": result['summary'],
            "subsector_rankings": [
                {
                    "subsector": subsector,
                    "data": {
                        "score": data['score'],
                        "frequency": data['frequency'],
                        "coverage": data['coverage'],
                        "sentiment": data['sentiment'],
                        "keywords": data['keywords'],
                        "stocks": data['stocks']
                    }
                }
                for subsector, data in result['subsector_rankings']
            ]
        }
        json.dump(serializable_result, f, ensure_ascii=False, indent=2)

    print(f"âœ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {output_file}")
    print(f"âœ“ JSONæ•°æ®å·²ä¿å­˜: {json_file}\n")

    # æ‰“å°æ‘˜è¦
    print("="*80)
    print("ç§‘æŠ€æ¿å—ç»†åˆ†é¢†åŸŸ TOP 10")
    print("="*80)
    print(f"{'æ’å':<6} {'ç»†åˆ†é¢†åŸŸ':<20} {'è¯„åˆ†':<8} {'æåŠæ¬¡æ•°':<10} {'è¦†ç›–ç‡':<10} {'æƒ…æ„Ÿ':<8}")
    print("-"*80)

    for i, (subsector, data) in enumerate(result['subsector_rankings'][:10], 1):
        print(f"{i:<6} {subsector:<20} {data['score']:<8} {data['frequency']:<10} "
              f"{data['coverage']}/13      {data['sentiment']:<8.2f}")

    print("="*80)

if __name__ == "__main__":
    main()
